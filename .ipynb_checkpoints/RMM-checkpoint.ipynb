{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12b005ce",
   "metadata": {},
   "source": [
    "## Testing RandMatrixMultiply"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cd3ff8",
   "metadata": {},
   "source": [
    "### Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e262ce2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# from scipy.sparse import random as sparse_random\n",
    "# from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b98b76c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def randMatrixMultiply(A: np.matrix, B: np.matrix, c: int, P_k: np.array) -> np.matrix:\n",
    "    \"\"\" This function returns an estimator for the product AB using the random matrix multiplication algorithm\n",
    "\n",
    "    :param A: The first m x n matrix to be multiplied\n",
    "    :param B: The second n x p matrix to be multiplied\n",
    "    :param c: The number of column-row pairs to choose\n",
    "    :param P_k: The probability distribution to choose the probability matrix\n",
    "    :return: The estimator for the product AB\n",
    "    \"\"\"\n",
    "    n = A.shape[1]\n",
    "    C = np.zeros((A.shape[0], c)) # m x c\n",
    "    R = np.zeros((c, B.shape[1])) # c x p\n",
    "    \n",
    "    # Optimal Probability Distribution:\n",
    "    A_col_norms = np.linalg.norm(A, axis=0)  \n",
    "    B_row_norms = np.linalg.norm(B, axis=1)  \n",
    "    P_k = (A_col_norms * B_row_norms) / np.sum(A_col_norms * B_row_norms)\n",
    "    \n",
    "    sorted_pk = np.flip(np.sort(P_k))\n",
    "    \n",
    "    indices = []\n",
    "    for i in range(len(sorted_pk)):\n",
    "        index = list(P_k).index(sorted_pk[i])\n",
    "        indices.append(index)\n",
    "    \n",
    "    for t in range(c): # For t = 0 to c-1\n",
    "        #i_t = np.random.choice(range(n), p=P_k)\n",
    "        i_t=indices[t]\n",
    "        coefficient = 1 / (np.sqrt(c * P_k[i_t]))\n",
    "        C[:, t] = coefficient * A[:, i_t]\n",
    "        R[t, :] = coefficient * B[i_t, :]\n",
    "        \n",
    "    return C @ R\n",
    "\n",
    "def calculate_accuracy(A: np.matrix, B: np.matrix, c: int, P_k: np.array) -> float:\n",
    "    \"\"\" This function calculates the accuracy of randMatrixMultiply compared to normal matrix multiplication\n",
    "\n",
    "    :param A: The first m x n matrix to be multiplied\n",
    "    :param B: The second n x p matrix to be multiplied\n",
    "    :param c: The number of column-row pairs to choose\n",
    "    :param P_k: The probability distribution to choose the probability matrix\n",
    "    :return: The accuracy of the randMatrixMultiply function\n",
    "    \"\"\"\n",
    "    # Compute the product using normal matrix multiplication\n",
    "    AB_exact = A @ B\n",
    "    \n",
    "    # Compute the product using randMatrixMultiply\n",
    "    AB_approx = randMatrixMultiply(A, B, c, P_k)\n",
    "    \n",
    "    # Calculate the Frobenius norm of the difference\n",
    "    difference = AB_exact - AB_approx\n",
    "    frobenius_norm = np.linalg.norm(difference, 'fro')\n",
    "    \n",
    "    # Calculate the Frobenius norm of the exact product\n",
    "    frobenius_norm_exact = np.linalg.norm(AB_exact, 'fro')\n",
    "    \n",
    "    # Calculate the accuracy as 1 - (norm of difference / norm of exact product)\n",
    "    accuracy = 1 - (frobenius_norm / frobenius_norm_exact)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "def calculate_loss(A: np.matrix, B: np.matrix, c: int, P_k: np.array) -> float:\n",
    "    \"\"\" This function calculates the loss of randMatrixMultiply compared to normal matrix multiplication\n",
    "\n",
    "    :param A: The first m x n matrix to be multiplied\n",
    "    :param B: The second n x p matrix to be multiplied\n",
    "    :param c: The number of column-row pairs to choose\n",
    "    :param P_k: The probability distribution to choose the probability matrix\n",
    "    :return: The loss of the randMatrixMultiply function\n",
    "    \"\"\"\n",
    "\n",
    "    AB_exact = A @ B\n",
    "    AB_approx = randMatrixMultiply(A, B, c, P_k)\n",
    "    # Calculate the Frobenius norm of the difference\n",
    "    difference = AB_exact - AB_approx\n",
    "    frobenius_norm = np.linalg.norm(difference, 'fro')\n",
    "    \n",
    "    return frobenius_norm/np.linalg.norm(AB_approx, 'fro')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882a9d4b",
   "metadata": {},
   "source": [
    "### Setting parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ba50341b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the dimensions of the matrices\n",
    "m = 10000\n",
    "n = 1000\n",
    "p = 500\n",
    "\n",
    "# # Create the matrices A and B\n",
    "A = np.random.poisson(size=(m, n))\n",
    "B = np.random.poisson(size=(n, p))\n",
    "\n",
    "# Choose the number of column-row pairs to choose\n",
    "c = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21a2cad",
   "metadata": {},
   "source": [
    "### Testing RandMatrixMultiply with uniform probability distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3ec0e6d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.053164463714623585\n"
     ]
    }
   ],
   "source": [
    "# for c in range(1,99):\n",
    "loss = calculate_loss(A, B, 500, [])\n",
    "    \n",
    "print(\"Loss: \"+str(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729163b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimal Probability Distribution:\n",
    "A_col_norms = np.linalg.norm(A, axis=0)  \n",
    "B_row_norms = np.linalg.norm(B, axis=1)  \n",
    "P_k = (A_col_norms * B_row_norms) / np.sum(A_col_norms * B_row_norms)\n",
    "\n",
    "losses = []\n",
    "for i in range(1000):\n",
    "    losses.append(calculate_loss(A, B, c, P_k))\n",
    "\n",
    "print(\"Mean: \"+str(np.mean(losses)))\n",
    "\n",
    "plt.title('Losses of Multiple trials of RMM algorithm (optimal prob dist)')\n",
    "plt.xlabel('Losses')\n",
    "plt.ylabel('Number of trials')\n",
    "plt.hist(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0198b0ca",
   "metadata": {},
   "source": [
    "### Effect of varying number of sampled row/column pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4909c9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "#Effect of varying c\n",
    "\n",
    "A_col_norms = np.linalg.norm(A, axis=0)  \n",
    "B_row_norms = np.linalg.norm(B, axis=1)  \n",
    "P_k = (A_col_norms * B_row_norms) / np.sum(A_col_norms * B_row_norms)\n",
    "\n",
    "averageaccs=[]\n",
    "for c in tqdm(range(5,100)):\n",
    "    losses = []\n",
    "    for i in range(10):\n",
    "        losses.append(calculate_loss(A, B, c, P_k))\n",
    "    averageaccs.append(np.mean(losses))\n",
    "\n",
    "plt.title('Average Loss for different c')\n",
    "plt.xlabel('c')\n",
    "plt.ylabel('average loss')\n",
    "plt.plot(range(5,100), averageaccs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0080ff30-98d6-4894-be43-620796b81f61",
   "metadata": {},
   "source": [
    "# RandNLA Approaches for Regression Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b066512e-44a2-400c-bfd3-02b337dbf990",
   "metadata": {},
   "source": [
    "### 5.1 The Randomized Hadamard Transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b758da-9f54-4963-bcc7-1001cbb50dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://numpy.org/doc/2.1/reference/random/generated/numpy.random.choice.html\n",
    "# show pratyush the above, which does uniform sampling w/ or w/o replacement for us O.o\n",
    "def randLeastSquares(A: np.matrix, b: np.array, epsilon: float):\n",
    "    \"\"\"\n",
    "    Solves the least squares problem using a randLeastSquares algorithm.\n",
    "\n",
    "    Parameters:\n",
    "    A (np.matrix): The input matrix of shape (n, d).\n",
    "    b (np.array): The target vector of shape (n).\n",
    "    epsilon (float): The error tolerance for the approximation.\n",
    "\n",
    "    Returns:\n",
    "    np.array: The solution vector of shape (d).\n",
    "    \"\"\"\n",
    "    n, d = A.shape\n",
    "    r = np.max(48**2 * d * np.log(40*n*d) * np.log(100**2 * d * np.log(40*n*d)), (40 * d * np.log(40 * n * d)) / epsilon)\n",
    "\n",
    "    # Creation of the samping-and-rescaling matrix S\n",
    "    S = np.zeros((r, d))\n",
    "    for t in range(r):\n",
    "        i_t = np.random.choice(range(n))\n",
    "        # for step 3, what is i? \n",
    "        e_i = np.zeros(n)\n",
    "        e_i[i_t] = np.sqrt(n/r)\n",
    "        S[t] = e_i @ A\n",
    "    \n",
    "    # normalized Random Hadamard Transform O(n log_2 r)\n",
    "    H =  np.multiply(n, n)/ np.sqrt(n)\n",
    "    D_diags = np.random.choice([-1, 1], size=n)\n",
    "    D = np.diag(D_diags)\n",
    "\n",
    "    # pinv = moore penrose pseudo inverse\n",
    "    HDA = H @ D @ A\n",
    "    HDb = H @ D @ b\n",
    "\n",
    "    x_opt = np.linalg.pinv(S.T @ HDA) @ (S.T @ HDb)\n",
    "    return x_opt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9d42ab-390f-4f8c-899e-c2531817821d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
